# CMPE297-ShortStory

[Medium Article Link](https://medium.com/@saipraneethk181200/reasoning-with-large-language-models-a-journey-into-the-mind-of-ai-2c6bc0e913d8)

[SlideShare Link](https://www.slideshare.net/slideshow/cmpe297-short-story-reasoning-with-llm-s/273318631)

[YouTube Link]()


---

# Reasoning with Large Language Models: A Journey into the Mind of AI

This repository holds the source code for an article exploring the fascinating capabilities of large language models (LLMs) in reasoning. This work draws heavily on the research documented in the survey paper, "Reasoning with Large Language Models, a Survey" [arXiv:2407.11511v1 [cs.AI] 16 Jul 2024] by Aske Plaat et al.

The article delves into the world of LLMs and their remarkable ability to reason, a groundbreaking development in the field of artificial intelligence. It highlights how LLMs have progressed from simple language processing tasks to tackling complex reasoning problems that were previously considered off-limits. 

**Key Takeaways:**

- **From Association to Reasoning:** LLMs have transcended simple association tasks, such as translation and summarization, to solve more complex problems like grade-school math word problems. 
- **The Chain-of-Thought Paradigm:**  The "Chain-of-Thought" prompt learning technique has revolutionized LLM reasoning. By encouraging the model to break down complex problems into a series of intermediate steps, researchers have significantly improved performance on challenging tasks.
- **A Taxonomy of Reasoning Approaches:** The article presents a comprehensive taxonomy of LLM reasoning approaches, grouping them based on prompt generation, step evaluation, and step control.
- **Promising Techniques:**  The article explores various techniques for enhancing LLM reasoning, including:
    - **Self-Assessment:** LLMs can assess the validity of their reasoning steps.
    - **Self-Verification:**  LLMs can revisit their reasoning steps, improving their accuracy. 
    - **Tool-Based Evaluation:** Formal languages, like Python, can be used for step evaluation, enabling more accurate assessment of reasoning.
    - **Dataset Augmentation:**  LLMs can learn from their own reasoning mistakes, improving their performance through dataset augmentation. 
    - **Ensemble Strategies:** Combining multiple reasoning paths can enhance accuracy and robustness. 
    - **Reinforcement Learning:**  LLMs can learn from their actions, optimizing reasoning chains through interactive feedback. 
- **The Future of LLM Reasoning:**  The article discusses limitations of LLMs, including their propensity for hallucination and their difficulty with truly metacognitive reasoning. It also explores promising avenues for future research, including:
    - **Improving prompt engineering:**  Developing more sophisticated prompts that guide LLMs through complex reasoning tasks.
    - **Enhancing grounding:**  Grounding LLM reasoning in real-world contexts to reduce hallucination and improve accuracy.
    - **Developing novel benchmarks:** Creating challenging benchmarks that push the boundaries of LLM reasoning capabilities.
    - **Exploring metacognition:** Understanding how LLMs can reflect on their own reasoning processes and learn from their mistakes. 
